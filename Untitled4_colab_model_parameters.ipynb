{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn2ICj-GBW7O"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy,categorical_accuracy,Mean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([[1.0,0.0,0.0],[0.0,1.0,0.0],[1.0,10.0,0.0],[0.0,0.0,1.0],[1.0,0.0,0.0],[0.0,1.0,0.0]])\n"
      ],
      "metadata": {
        "id": "GjcjZvGtCDCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.random.rand(6,3)"
      ],
      "metadata": {
        "id": "SQcC47F-CugW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_accuracy(y_true,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un5Wga6_DOdv",
        "outputId": "9e6d236f-d33f-4570-9762-20663f0e1ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 0., 0., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(np.argmax(y_true,axis=-1) == np.argmax(y_test,axis=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74TKE2R5DSy7",
        "outputId": "2b1ef839-b783-460d-d12e-181acad187af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(y_test,axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxKDd0SQDYkD",
        "outputId": "cae0ce60-6e13-468b-83e5-96fc57b0a11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 2, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mean()"
      ],
      "metadata": {
        "id": "dt9NoM61DsrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.update_state(categorical_accuracy(y_true,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1FdWbfvDcEV",
        "outputId": "32896a9f-5a24-4e55-8e2a-840d6ebd2d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=6.0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgBmquk9DiuZ",
        "outputId": "7bc3b988-64d8-4ca6-b07d-e28e62eb7c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.16666667>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.reset_state()"
      ],
      "metadata": {
        "id": "-3cQpVOyEidc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc= 0\n",
        "i = 0"
      ],
      "metadata": {
        "id": "GrgjnHsiEEPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc += np.mean(categorical_accuracy(y_true,y_test))\n",
        "i += 1"
      ],
      "metadata": {
        "id": "GHojWSrxEGYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc / i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxi_h64EGbY",
        "outputId": "585278f1-6615-4e8e-dd85-3c42becf4ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1666666716337204"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(categorical_accuracy(y_true,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmbI3q1EDx-G",
        "outputId": "a07044bb-7048-4993-f759-f14dcda31f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(View, self).__init__()\n",
        "   \n",
        "    def forward(self, x):\n",
        "        numel = x.numel() / x.shape[0]\n",
        "        return x.view(-1, int(numel)) \n",
        "\n",
        "def convNoutput(convs, input_size): # predict output size after conv layers\n",
        "    input_size = int(input_size)\n",
        "    input_channels = convs[0][1].weight.shape[1] # input channel\n",
        "    output = torch.Tensor(1, input_channels, input_size, input_size)\n",
        "    with torch.no_grad():\n",
        "        for conv in convs:\n",
        "            output = conv(output)\n",
        "    return output.numel(), output.shape\n",
        "\n",
        "class stn(nn.Module):\n",
        "    def __init__(self, input_channels, input_size, params):\n",
        "        super(stn, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv1 = nn.Sequential(\n",
        "                    nn.ReplicationPad2d(2),\n",
        "                    nn.Conv2d(input_channels, params[0], kernel_size=5, stride=1),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "        self.conv2 = nn.Sequential(\n",
        "                    nn.ReplicationPad2d(2),\n",
        "                    nn.Conv2d(params[0], params[1], kernel_size=5, stride=1),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "                    \n",
        "        self.conv3 = nn.Sequential(\n",
        "                    nn.ReplicationPad2d(2),\n",
        "                    nn.Conv2d(params[1], params[2], kernel_size=3, stride=1),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "\n",
        "        out_numel, out_size = convNoutput([self.conv1, self.conv2, self.conv3], input_size/2)\n",
        "        # set fc layer based on predicted size\n",
        "        self.fc = nn.Sequential(\n",
        "                View(),\n",
        "                nn.Linear(out_numel, params[3]),\n",
        "                nn.ReLU()\n",
        "                )\n",
        "        self.classifier = classifier = nn.Sequential(\n",
        "                View(),\n",
        "                nn.Linear(params[3], 6) # affine transform has 6 parameters\n",
        "                )\n",
        "        # initialize stn parameters (affine transform)\n",
        "        self.classifier[1].weight.data.fill_(0)\n",
        "        self.classifier[1].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
        "\n",
        "    def localization_network(self, x):\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        theta = self.localization_network(x)\n",
        "        theta = theta.view(-1,2,3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "        \n",
        "\n",
        "class SillNet(nn.Module):\n",
        "    def __init__(self, nc, input_size, class_train, class_test, extract_chn=None, classify_chn=None, param1=None, param2=None, param3=None, param4=None, param_mask=None):\n",
        "        super(SillNet, self).__init__()\n",
        "\n",
        "        self.extract_chn = extract_chn\n",
        "        self.classify_chn = classify_chn\n",
        "        self.param1 = param1\n",
        "        self.param2 = param2\n",
        "        self.param3 = param3\n",
        "        self.param4 = param4\n",
        "        self.input_size = input_size\n",
        "        self.nc = nc\n",
        "        self.class_train = class_train\n",
        "        self.class_test = class_test\n",
        "\n",
        "        # extracter\n",
        "        self.ex_pd1 = nn.ReplicationPad2d(2)\n",
        "        self.ex1 = nn.Conv2d(nc, self.extract_chn[0], 5, 1) # inchn, outchn, kernel, stride, padding, dilation, groups\n",
        "        self.ex_bn1 = nn.InstanceNorm2d(self.extract_chn[0])\n",
        "\n",
        "        self.ex_pd2 = nn.ReplicationPad2d(2)\n",
        "        self.ex2 = nn.Conv2d(self.extract_chn[0], self.extract_chn[1], 5, 1) # 1/1\n",
        "        self.ex_bn2 = nn.InstanceNorm2d(self.extract_chn[1])\n",
        "\n",
        "        self.ex_pd3 = nn.ReplicationPad2d(1)\n",
        "        self.ex3 = nn.Conv2d(self.extract_chn[1], self.extract_chn[2], 3, 1) # 1/1\n",
        "        self.ex_bn3 = nn.InstanceNorm2d(self.extract_chn[2])\n",
        "        \n",
        "        self.ex_pd4 = nn.ReplicationPad2d(1)\n",
        "        self.ex4 = nn.Conv2d(self.extract_chn[2], self.extract_chn[3], 3, 1) # 1/1\n",
        "        self.ex_bn4 = nn.InstanceNorm2d(self.extract_chn[3])\n",
        "        \n",
        "        self.ex_pd5 = nn.ReplicationPad2d(1)\n",
        "        self.ex5 = nn.Conv2d(self.extract_chn[3], self.extract_chn[4], 3, 1) # 1/1\n",
        "        self.ex_bn5 = nn.InstanceNorm2d(self.extract_chn[4])\n",
        "        \n",
        "        \n",
        "        self.ex_pd6 = nn.ReplicationPad2d(1)\n",
        "        self.ex6 = nn.Conv2d(self.extract_chn[4], self.extract_chn[5], 3, 1) # 1/1\n",
        "        self.ex_bn6 = nn.InstanceNorm2d(self.extract_chn[5])\n",
        "        \n",
        "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        # decoder\n",
        "        self.up1 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "        self.de_pd1 = nn.ReplicationPad2d(1)\n",
        "        self.de1 = nn.Conv2d(int(self.extract_chn[5]/2), self.extract_chn[4], 3, 1)\n",
        "        self.de_bn1 = nn.InstanceNorm2d(self.extract_chn[4], 1.e-3)\n",
        "\n",
        "        self.de_pd2 = nn.ReplicationPad2d(1)\n",
        "        self.de2 = nn.Conv2d(self.extract_chn[4], self.extract_chn[3], 3, 1)\n",
        "        self.de_bn2 = nn.InstanceNorm2d(self.extract_chn[3], 1.e-3)\n",
        "        \n",
        "        self.de_pd3 = nn.ReplicationPad2d(1)\n",
        "        self.de3 = nn.Conv2d(self.extract_chn[3], self.extract_chn[2], 3, 1)\n",
        "        self.de_bn3 = nn.InstanceNorm2d(self.extract_chn[2], 1.e-3)\n",
        "        \n",
        "        self.de_pd4 = nn.ReplicationPad2d(1)\n",
        "        self.de4 = nn.Conv2d(self.extract_chn[2], self.extract_chn[1], 3, 1)\n",
        "        self.de_bn4 = nn.InstanceNorm2d(self.extract_chn[1], 1.e-3)\n",
        "\n",
        "        self.de_pd5 = nn.ReplicationPad2d(1)\n",
        "        self.de5 = nn.Conv2d(self.extract_chn[1], nc, 3, 1)\n",
        "        \n",
        "        # warping\n",
        "        if param1 is not None:\n",
        "            self.stn1 = stn(nc, self.input_size, param1)\n",
        "        if param2 is not None:\n",
        "            self.stn2 = stn(self.extract_chn[1], self.input_size, param2)\n",
        "        if param3 is not None:\n",
        "            self.stn3 = stn(self.extract_chn[3], self.input_size, param3)\n",
        "        if param4 is not None:\n",
        "            self.stn4 = stn(int(self.extract_chn[5]/2), self.input_size, param4)\n",
        "\n",
        "        # classifier 1\n",
        "        self.cls1 = nn.Conv2d(int(self.extract_chn[5]), self.classify_chn[0], 5, 1, 2) # inchn, outchn, kernel, stride, padding, dilation, groups                                \n",
        "        self.cls_bn1 = nn.BatchNorm2d(self.classify_chn[0])\n",
        "\n",
        "        self.cls2 = nn.Conv2d(self.classify_chn[0], self.classify_chn[1], 5, 1, 2) # 1/2\n",
        "        self.cls_bn2 = nn.BatchNorm2d(self.classify_chn[1])\n",
        "\n",
        "        self.cls3 = nn.Conv2d(self.classify_chn[1], self.classify_chn[2], 5, 1, 2) # 1/4\n",
        "        self.cls_bn3 = nn.BatchNorm2d(self.classify_chn[2])\n",
        "        \n",
        "        \n",
        "        self.cls4 = nn.Conv2d(self.classify_chn[2], self.classify_chn[3], 3, 1, 1) # 1/4\n",
        "        self.cls_bn4 = nn.BatchNorm2d(self.classify_chn[3])\n",
        "        \n",
        "        self.cls5 = nn.Conv2d(self.classify_chn[3], self.classify_chn[4], 3, 1, 1) # 1/8\n",
        "        self.cls_bn5 = nn.BatchNorm2d(self.classify_chn[4])\n",
        "        \n",
        "        self.cls6 = nn.Conv2d(self.classify_chn[4], self.classify_chn[5], 3, 1, 1) # 1/8\n",
        "        self.cls_bn6 = nn.BatchNorm2d(self.classify_chn[5])\n",
        "        \n",
        "        self.fc1 = nn.Linear(int(self.input_size/8*self.input_size/8)*self.classify_chn[5], self.class_train)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        \n",
        "        # classifier 2\n",
        "        self.cls21 = nn.Conv2d(int(self.extract_chn[5]), self.classify_chn[0], 5, 1, 2) # inchn, outchn, kernel, stride, padding, dilation, groups                                \n",
        "        self.cls2_bn1 = nn.BatchNorm2d(self.classify_chn[0])\n",
        "\n",
        "        self.cls22 = nn.Conv2d(self.classify_chn[0], self.classify_chn[1], 5, 1, 2) # 1/2\n",
        "        self.cls2_bn2 = nn.BatchNorm2d(self.classify_chn[1])\n",
        "\n",
        "        self.cls23 = nn.Conv2d(self.classify_chn[1], self.classify_chn[2], 5, 1, 2) # 1/4\n",
        "        self.cls2_bn3 = nn.BatchNorm2d(self.classify_chn[2])\n",
        "        \n",
        "        \n",
        "        self.cls24 = nn.Conv2d(self.classify_chn[2], self.classify_chn[3], 3, 1, 1) # 1/4\n",
        "        self.cls2_bn4 = nn.BatchNorm2d(self.classify_chn[3])\n",
        "        \n",
        "        self.cls25 = nn.Conv2d(self.classify_chn[3], self.classify_chn[4], 3, 1, 1) # 1/8\n",
        "        self.cls2_bn5 = nn.BatchNorm2d(self.classify_chn[4])\n",
        "        \n",
        "        self.cls26 = nn.Conv2d(self.classify_chn[4], self.classify_chn[5], 3, 1, 1) # 1/8\n",
        "        self.cls2_bn6 = nn.BatchNorm2d(self.classify_chn[5])       \n",
        "        \n",
        "        self.fc2 = nn.Linear(int(self.input_size/8*self.input_size/8)*self.classify_chn[5], self.class_test)\n",
        "        \n",
        "\n",
        "    def extract(self, x, is_warping):\n",
        "        if is_warping and self.param1 is not None:\n",
        "            x = self.stn1(x)\n",
        "        h1 = self.leakyrelu(self.ex_bn1(self.ex1(self.ex_pd1(x))))\n",
        "        h2 = self.leakyrelu(self.ex_bn2(self.ex2(self.ex_pd2(h1))))\n",
        "        \n",
        "        if is_warping and self.param2 is not None:\n",
        "            h2 = self.stn2(h2)\n",
        "        h3 = self.leakyrelu(self.ex_bn3(self.ex3(self.ex_pd3(h2))))\n",
        "        h4 = self.leakyrelu(self.ex_bn4(self.ex4(self.ex_pd4(h3))))\n",
        "        \n",
        "        if is_warping and self.param3 is not None:\n",
        "            h4 = self.stn3(h4)\n",
        "        h5 = self.leakyrelu(self.ex_bn5(self.ex5(self.ex_pd5(h4))))\n",
        "        h6 = self.sigmoid(self.ex_bn6(self.ex6(self.ex_pd6(h5))))\n",
        "        \n",
        "        feat_sem, feat_illu = torch.chunk(h6, 2, 1)\n",
        "        feat_sem_nowarp = feat_sem\n",
        "            \n",
        "        if is_warping and self.param4 is not None:\n",
        "            feat_sem = self.stn4(feat_sem)\n",
        "        \n",
        "        \n",
        "        return feat_sem, feat_illu, feat_sem_nowarp\n",
        "\n",
        "    def decode(self, x):\n",
        "        h1 = self.leakyrelu(self.de_bn1(self.de1(self.de_pd1(x))))\n",
        "        h2 = self.leakyrelu(self.de_bn2(self.de2(self.de_pd2(h1))))\n",
        "        h3 = self.leakyrelu(self.de_bn3(self.de3(self.de_pd3(h2))))\n",
        "        h4 = self.leakyrelu(self.de_bn4(self.de4(self.de_pd4(h3))))\n",
        "        out = self.sigmoid(self.de5(self.de_pd5(h4)))\n",
        "        return out\n",
        "        \n",
        "    def classify(self, x):\n",
        "        h1 = self.pool2(self.leakyrelu(self.cls_bn1(self.cls1(x))))\n",
        "        h2 = self.leakyrelu(self.cls_bn2(self.cls2(h1)))\n",
        "        h3 = self.pool2(self.leakyrelu(self.cls_bn3(self.cls3(h2))))\n",
        "        h4 = self.leakyrelu(self.cls_bn4(self.cls4(h3)))\n",
        "        h5 = self.pool2(self.leakyrelu(self.cls_bn5(self.cls5(h4))))\n",
        "        h6 = self.leakyrelu(self.cls_bn6(self.cls6(h5)))\n",
        "        h7 = h6.view(-1, int(self.input_size/8*self.input_size/8*self.classify_chn[5]))\n",
        "        out = self.fc1(h7)\n",
        "        return out\n",
        "    \n",
        "    def classify2(self, x):\n",
        "        h1 = self.pool2(self.leakyrelu(self.cls2_bn1(self.cls21(x))))\n",
        "        h2 = self.leakyrelu(self.cls2_bn2(self.cls22(h1)))\n",
        "        h3 = self.pool2(self.leakyrelu(self.cls2_bn3(self.cls23(h2))))\n",
        "        h4 = self.leakyrelu(self.cls2_bn4(self.cls24(h3)))\n",
        "        h5 = self.pool2(self.leakyrelu(self.cls2_bn5(self.cls25(h4))))\n",
        "        h6 = self.leakyrelu(self.cls2_bn6(self.cls26(h5)))\n",
        "        h7 = h6.view(-1, int(self.input_size/8*self.input_size/8*self.classify_chn[5]))\n",
        "        out = self.fc2(h7)\n",
        "        return out\n",
        "\n",
        "    def init_params(self, net):\n",
        "        print('Loading the model from the file...')\n",
        "        net_dict = self.state_dict()\n",
        "        if isinstance(net, dict):\n",
        "            pre_dict = net\n",
        "        else:\n",
        "            pre_dict = net.state_dict()\n",
        "        # 1. filter out unnecessary keys\n",
        "        pre_dict = {k: v for k, v in pre_dict.items() if (k in net_dict)}\n",
        "        net_dict.update(pre_dict)\n",
        "        # 3. load the new state dict\n",
        "        self.load_state_dict(net_dict)\n"
      ],
      "metadata": {
        "id": "7ZcxmjQQD3vO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SillNet(nc=3, input_size = 64, class_train=43, class_test = 36, extract_chn=[100, 150, 200, 150, 100, 6], classify_chn = [100, 150, 200, 250, 300, 100], param1 = None, param2 = None, param3 = None, param4 = [150,150,150,150])"
      ],
      "metadata": {
        "id": "QMUdtoRyHqi1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItQiJ1sHs92",
        "outputId": "1a24ffd9-55bb-4568-a773-74995a5b694b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f9f73255270>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp"
      ],
      "metadata": {
        "id": "hwAN40HJJNIO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_n_params(clf) / 1e6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNd4jwkLJT6C",
        "outputId": "561de319-8381-4427-e6ac-747bc9e5a46b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.668494"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gtsrb = SillNet(nc=3, input_size = 64, class_train=43, class_test = 43, extract_chn=[150, 150, 150, 150, 150, 6], classify_chn = [100, 150, 200, 150, 100, 100, 100], param1 = None, param2 = [150,150,150,150], param3 = [150,150,150,150], param4 = [150,150,150,150])"
      ],
      "metadata": {
        "id": "DBt06mcfJV4b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_n_params(gtsrb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV1qtLLrJ1V_",
        "outputId": "19a53f1b-9f94-4369-8adf-e24275aafa51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10754863"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "RNn8ocY2J3pS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(clf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4hqQx1jKAGh",
        "outputId": "635838b6-2bd4-4083-e5fa-90bcbd1db606"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8668494"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is modified from the repository\n",
        "# https://github.com/bhpfelix/Variational-Autoencoder-PyTorch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(View, self).__init__()\n",
        "   \n",
        "    def forward(self, x):\n",
        "        numel = x.numel() / x.shape[0]\n",
        "        return x.view(-1, int(numel)) \n",
        "\n",
        "def convNoutput(convs, input_size): # predict output size after conv layers\n",
        "    input_size = int(input_size)\n",
        "    input_channels = convs[0][0].weight.shape[1] # input channel\n",
        "    output = torch.Tensor(1, input_channels, input_size, input_size)\n",
        "    with torch.no_grad():\n",
        "        for conv in convs:\n",
        "            output = conv(output)\n",
        "    return output.numel(), output.shape\n",
        "\n",
        "class stn(nn.Module):\n",
        "    def __init__(self, input_channels, input_size, params):\n",
        "        super(stn, self).__init__()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(input_channels, params[0], kernel_size=5, stride=1, padding=2),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "        self.conv2 = nn.Sequential(\n",
        "                    nn.Conv2d(params[0], params[1], kernel_size=5, stride=1, padding=2),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "\n",
        "        out_numel, out_size = convNoutput([self.conv1, self.conv2], input_size/2)\n",
        "        # set fc layer based on predicted size\n",
        "        self.fc = nn.Sequential(\n",
        "                View(),\n",
        "                nn.Linear(out_numel, params[2]),\n",
        "                nn.ReLU()\n",
        "                )\n",
        "        self.classifier = classifier = nn.Sequential(\n",
        "                View(),\n",
        "                nn.Linear(params[2], 6) # affine transform has 6 parameters\n",
        "                )\n",
        "        # initialize stn parameters (affine transform)\n",
        "        self.classifier[1].weight.data.fill_(0)\n",
        "        self.classifier[1].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
        "\n",
        "    def localization_network(self, x):\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        theta = self.localization_network(x)\n",
        "        theta = theta.view(-1,2,3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAEIdsiae(nn.Module):\n",
        "    def __init__(self, nc, input_size, latent_variable_size=300, cnn_chn=[100, 150, 250], param1=None, param2=None, param3=None):\n",
        "        super(VAEIdsiae, self).__init__()\n",
        "\n",
        "        self.cnn_chn = cnn_chn\n",
        "        self.param1 = param1\n",
        "        self.param2 = param2\n",
        "        self.param3 = param3\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.nc = nc\n",
        "        self.latent_variable_size = latent_variable_size\n",
        "\n",
        "        # encoder\n",
        "        self.e1 = nn.Conv2d(nc, self.cnn_chn[0], 7, 2, 3) # inchn, outchn, kernel, stride, padding, dilation, groups\n",
        "        self.bn1 = nn.BatchNorm2d(self.cnn_chn[0])\n",
        "\n",
        "        self.e2 = nn.Conv2d(self.cnn_chn[0], self.cnn_chn[1], 4, 2, 1) # 1/4\n",
        "        self.bn2 = nn.BatchNorm2d(self.cnn_chn[1])\n",
        "\n",
        "        self.e3 = nn.Conv2d(self.cnn_chn[1], self.cnn_chn[2], 4, 2, 1) # 1/8\n",
        "        self.bn3 = nn.BatchNorm2d(self.cnn_chn[2])\n",
        "\n",
        "        self.fc1 = nn.Linear(int(input_size/8*input_size/8*self.cnn_chn[2]), latent_variable_size)\n",
        "        self.fc2 = nn.Linear(int(input_size/8*input_size/8*self.cnn_chn[2]), latent_variable_size)\n",
        "\n",
        "        # decoder\n",
        "\n",
        "        if param1 is not None:\n",
        "            self.stn1 = stn(3, self.input_size, param1)\n",
        "        if param2 is not None:\n",
        "            self.stn2 = stn(self.cnn_chn[0], self.input_size/2, param2)\n",
        "        if param3 is not None:\n",
        "            self.stn3 = stn(self.cnn_chn[1], self.input_size/4, param3)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        if self.param1 is not None:\n",
        "            x = self.stn1(x)\n",
        "\n",
        "        h1 = self.leakyrelu(self.bn1(self.e1(x)))\n",
        "        if self.param2 is not None:\n",
        "            h1 = self.stn2(h1)\n",
        "\n",
        "        h2 = self.leakyrelu(self.bn2(self.e2(h1)))\n",
        "        if self.param3 is not None:\n",
        "            h2 = self.stn3(h2)\n",
        "\n",
        "        h3 = self.leakyrelu(self.bn3(self.e3(h2)))\n",
        "        h4 = h3.view(-1, int(self.input_size/8*self.input_size/8*self.cnn_chn[2]))\n",
        "\n",
        "        return self.fc1(h4), self.fc2(h4), x\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def get_latent_var(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar, xstn = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        res = 0\n",
        "        #res = self.decode(z)\n",
        "        return res, mu, logvar, xstn\n",
        "\n",
        "    def init_params(self, net):\n",
        "        print('Loading the model from the file...')\n",
        "        net_dict = self.state_dict()\n",
        "        if isinstance(net, dict):\n",
        "            pre_dict = net\n",
        "        else:\n",
        "            pre_dict = net.state_dict()\n",
        "        # 1. filter out unnecessary keys\n",
        "        pre_dict = {k: v for k, v in pre_dict.items() if (k in net_dict)} # for fs net\n",
        "        net_dict.update(pre_dict)\n",
        "        # 3. load the new state dict\n",
        "        self.load_state_dict(net_dict)"
      ],
      "metadata": {
        "id": "sV6tDoVWKCf-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_vpe_encoder = VAEIdsiae(nc=3, input_size = 64, latent_variable_size=300, cnn_chn=[100, 150, 250] ,param1=[200,300,200], param2=None, param3 = [150, 150, 150])"
      ],
      "metadata": {
        "id": "Tf6yfIqEKZuN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(clf_vpe_encoder) / 1e6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oU3yKaxLOGr",
        "outputId": "c81c6c5e-f88b-4ac3-97e2-cd4fdf9fe9c6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.030062"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is modified from the repository\n",
        "# https://github.com/bhpfelix/Variational-Autoencoder-PyTorch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(View, self).__init__()\n",
        "   \n",
        "    def forward(self, x):\n",
        "        numel = x.numel() / x.shape[0]\n",
        "        return x.view(-1, int(numel)) \n",
        "\n",
        "def convNoutput(convs, input_size): # predict output size after conv layers\n",
        "    input_size = int(input_size)\n",
        "    input_channels = convs[0][0].weight.shape[1] # input channel\n",
        "    output = torch.Tensor(1, input_channels, input_size, input_size)\n",
        "    with torch.no_grad():\n",
        "        for conv in convs:\n",
        "            output = conv(output)\n",
        "    return output.numel(), output.shape\n",
        "\n",
        "class stn(nn.Module):\n",
        "    def __init__(self, input_channels, input_size, params):\n",
        "        super(stn, self).__init__()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv1 = nn.Sequential(\n",
        "                    nn.Conv2d(input_channels, params[0], kernel_size=5, stride=1, padding=2),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "        self.conv2 = nn.Sequential(\n",
        "                    nn.Conv2d(params[0], params[1], kernel_size=5, stride=1, padding=2),\n",
        "                    nn.ReLU(True),\n",
        "                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                    )\n",
        "\n",
        "        out_numel, out_size = convNoutput([self.conv1, self.conv2], input_size/2)\n",
        "        # set fc layer based on predicted size\n",
        "        self.fc = nn.Sequential(\n",
        "                View(),\n",
        "                nn.Linear(out_numel, params[2]),\n",
        "                nn.ReLU()\n",
        "                )\n",
        "        self.classifier = classifier = nn.Sequential(\n",
        "                View(),\n",
        "                nn.Linear(params[2], 6) # affine transform has 6 parameters\n",
        "                )\n",
        "        # initialize stn parameters (affine transform)\n",
        "        self.classifier[1].weight.data.fill_(0)\n",
        "        self.classifier[1].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
        "\n",
        "    def localization_network(self, x):\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        theta = self.localization_network(x)\n",
        "        theta = theta.view(-1,2,3)\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAEIdsia(nn.Module):\n",
        "    def __init__(self, nc, input_size, latent_variable_size=300, cnn_chn=[100, 150, 250], param1=None, param2=None, param3=None):\n",
        "        super(VAEIdsia, self).__init__()\n",
        "\n",
        "        self.cnn_chn = cnn_chn\n",
        "        self.param1 = param1\n",
        "        self.param2 = param2\n",
        "        self.param3 = param3\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.nc = nc\n",
        "        self.latent_variable_size = latent_variable_size\n",
        "\n",
        "        # encoder\n",
        "        self.e1 = nn.Conv2d(nc, self.cnn_chn[0], 7, 2, 3) # inchn, outchn, kernel, stride, padding, dilation, groups\n",
        "        self.bn1 = nn.BatchNorm2d(self.cnn_chn[0])\n",
        "\n",
        "        self.e2 = nn.Conv2d(self.cnn_chn[0], self.cnn_chn[1], 4, 2, 1) # 1/4\n",
        "        self.bn2 = nn.BatchNorm2d(self.cnn_chn[1])\n",
        "\n",
        "        self.e3 = nn.Conv2d(self.cnn_chn[1], self.cnn_chn[2], 4, 2, 1) # 1/8\n",
        "        self.bn3 = nn.BatchNorm2d(self.cnn_chn[2])\n",
        "\n",
        "        self.fc1 = nn.Linear(int(input_size/8*input_size/8*self.cnn_chn[2]), latent_variable_size)\n",
        "        self.fc2 = nn.Linear(int(input_size/8*input_size/8*self.cnn_chn[2]), latent_variable_size)\n",
        "\n",
        "        # decoder\n",
        "        self.d1 = nn.Linear(latent_variable_size, int(input_size/8*input_size/8*self.cnn_chn[2]))\n",
        "\n",
        "        self.up1 = nn.UpsamplingNearest2d(scale_factor=2) # 8 -> 16\n",
        "        self.pd1 = nn.ReplicationPad2d(1)\n",
        "        self.d2 = nn.Conv2d(self.cnn_chn[2], self.cnn_chn[1], 3, 1)\n",
        "        self.bn6 = nn.BatchNorm2d(self.cnn_chn[1], 1.e-3)\n",
        "\n",
        "        self.up2 = nn.UpsamplingNearest2d(scale_factor=2) # 16 -> 32\n",
        "        self.pd2 = nn.ReplicationPad2d(1)\n",
        "        self.d3 = nn.Conv2d(self.cnn_chn[1], self.cnn_chn[0], 3, 1)\n",
        "        self.bn7 = nn.BatchNorm2d(self.cnn_chn[0], 1.e-3)\n",
        "\n",
        "        self.up3 = nn.UpsamplingNearest2d(scale_factor=2) # 32 -> 64\n",
        "        self.pd3 = nn.ReplicationPad2d(1)\n",
        "        self.d4 = nn.Conv2d(self.cnn_chn[0], 3, 3, 1)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        if param1 is not None:\n",
        "            self.stn1 = stn(3, self.input_size, param1)\n",
        "        if param2 is not None:\n",
        "            self.stn2 = stn(self.cnn_chn[0], self.input_size/2, param2)\n",
        "        if param3 is not None:\n",
        "            self.stn3 = stn(self.cnn_chn[1], self.input_size/4, param3)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        if self.param1 is not None:\n",
        "            x = self.stn1(x)\n",
        "\n",
        "        h1 = self.leakyrelu(self.bn1(self.e1(x)))\n",
        "        if self.param2 is not None:\n",
        "            h1 = self.stn2(h1)\n",
        "\n",
        "        h2 = self.leakyrelu(self.bn2(self.e2(h1)))\n",
        "        if self.param3 is not None:\n",
        "            h2 = self.stn3(h2)\n",
        "\n",
        "        h3 = self.leakyrelu(self.bn3(self.e3(h2)))\n",
        "        h4 = h3.view(-1, int(self.input_size/8*self.input_size/8*self.cnn_chn[2]))\n",
        "\n",
        "        return self.fc1(h4), self.fc2(h4), x\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h1 = self.relu(self.d1(z))\n",
        "        # h1 = h1.view(-1, self.ngf*8*2, 4, 4)\n",
        "        h1 = h1.view(-1, self.cnn_chn[2], int(self.input_size/8), int(self.input_size/8))\n",
        "        h2 = self.leakyrelu(self.bn6(self.d2(self.pd1(self.up1(h1)))))\n",
        "        h3 = self.leakyrelu(self.bn7(self.d3(self.pd2(self.up2(h2)))))\n",
        "        return self.sigmoid(self.d4(self.pd3(self.up3(h3))))\n",
        "\n",
        "    def get_latent_var(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar, xstn = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        res = self.decode(z)\n",
        "        return res, mu, logvar, xstn\n",
        "\n",
        "    def init_params(self, net):\n",
        "        print('Loading the model from the file...')\n",
        "        net_dict = self.state_dict()\n",
        "        if isinstance(net, dict):\n",
        "            pre_dict = net\n",
        "        else:\n",
        "            pre_dict = net.state_dict()\n",
        "        # 1. filter out unnecessary keys\n",
        "        pre_dict = {k: v for k, v in pre_dict.items() if (k in net_dict)} # for fs net\n",
        "        net_dict.update(pre_dict)\n",
        "        # 3. load the new state dict\n",
        "        self.load_state_dict(net_dict)\n"
      ],
      "metadata": {
        "id": "d-8cUShaLShJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_vpe = VAEIdsia(nc=3, input_size = 64, latent_variable_size=300, cnn_chn=[100, 150, 250] ,param1=[200,300,200], param2=None, param3 = [150, 150, 150])"
      ],
      "metadata": {
        "id": "Y5MJSK7PMAgP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(clf_vpe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9C39TP7NISG",
        "outputId": "45883607-2939-441e-c1a6-a7830e87465e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22322015"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AztK7wpyNN00"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}